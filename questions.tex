\section{Research Questions}
\label{sec:questions}

As Internet discourse and content becomes increasingly consolidated, a
relatively small number of platforms now have disproportionate decision-making
power over how content is hosted and disseminated on the Internet. The
practice of deciding whether to publish, remove, or flag content that is
posted by third-party users is typically referred to as {\em content
moderation}.  In certain contexts such as copyright, the concept of content
moderation is well-established. On the other hand, over the past several
decades, the scope of content moderation has expanded considerably, to include
a range of speech, including terrorist speech, hate speech, harassment,
disinformation, and more recently, AI generated content.  In many of these
cases, laws and regulations are less well-established. Furthermore, the
technical capabilities to detect forms of content and speech that run afoul of
norms and platform terms of service (in the absence of laws and regulations)
are also less mature. To compound the problem, the amount of content that
users post to platforms has increased by several orders of magnitude.  Such
scale is incredibly difficult to deal with, particularly for platforms with
fewer employees or operational budgets to invest in complex and expensive
content moderation systems. At the backdrop of this rapidly evolving ecosystem
are platform policies, proposed legislation, regulation, and laws that may not
always square with the technical realities or usersâ€™ understanding of these
policies. In light of this evolving---and relatively immature---ecosystem, the
goal of this project is to understand this ecosystem better, from the
perspective of platform policies (i.e., rules). 

To this end, in this project, we aim to study three questions: 
\begin{enumerate}
\item {\bf What is the landscape of current content moderation policies?
        (Section~\ref{sec:landscape}).} We propose to build the first
        public, large-scale, longitudinal, annotated corpus of content moderation
        rules across platforms. 
        We have done preliminary work to build a first version of such corpus and the associated annotations.
        We will expand our preliminary analyses and perform continuous measurements to track
        how these policies change over time. 
        Building on this corpus, we will study whether a platform's stated policies are consistent with both user
        expectation (Section~\ref{sec:user}) and observed practice
        (Section~\ref{sec:auditing}).
    \item {\bf How do users perceive, understand, and react to content
        moderation? (Section~\ref{sec:user}).} One important aspect of a
        trustworthy content moderation system is a user's ability to
        understand a platform's policies and rules. If a platform's policies
        are perceived as inconsistent or arbitrary, the platform may be deemed
        untrustworthy. Furthermore, as previous research has shown~\cite{fiesler_understanding_2015},
        inconsistent behavior concerning moderation and censorship can also
        result in chilling effects, where users self-censor. Thus, it is
        important to understand how users perceive and react to the stated
        policies and rules of different platforms. We will conduct surveys and interviews to gage user understanding of current content
        moderation rules and expectations concerning the enforcement of
        these rules. Content used in the user studies will be drawn
        from the corpus of content that we build and curate in
        Section~\ref{sec:landscape} and~\ref{sec:auditing}.
    \item {\bf How do different platforms implement content moderation in
        practice? (Section~\ref{sec:auditing})} In addition to having policies
        that are consistent with user norms and expectations, a second important aspect of a
        trustworthy content moderation system is predictable, consistent
        application of policies. To this end, we aim to perform continual,
        large-scale measurements of the content moderation practices of
        several online platforms, for a range of different types
        of content. We will perform 
        continual observational measurement studies to observe content moderation behavior
        of different platforms and controlled audit studies where possible.
\end{enumerate}
\noindent
By performing continual, large-scale measurements of the content
moderation practices, we will study a range of popular online
platforms with user-generated content that vary in terms of type of
media, type of content, and scale of platform. This range of platforms
will allow us to understand how moderation policies (and how users perceive,
understand, and react to those policies) differ across platforms. 
